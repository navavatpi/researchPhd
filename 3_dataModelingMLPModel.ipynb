{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3_dataModelingMLPModel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPf1y9C2thGVyWDu3ouM9+U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/navavatpi/researchPhd/blob/developANN/3_dataModelingMLPModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riTfccSRae30"
      },
      "source": [
        "# 3. Data Modeling: MLP model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxqLVbvJamze"
      },
      "source": [
        "## Environment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8Q8NLwYNWIr"
      },
      "source": [
        "## TPU backend"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQME0BwnuXWU"
      },
      "source": [
        "# # Enabling and testing the TPU\n",
        "# %tensorflow_version 2.x\n",
        "# import tensorflow as tf\n",
        "# print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "# try:\n",
        "#   tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "#   print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "# except ValueError:\n",
        "#   raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "# tf.config.experimental_connect_to_cluster(tpu)\n",
        "# tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "# tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "\n",
        "# # install scikit-plot to Colab\n",
        "# !pip install scikit-plot"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLwY5UWTNSTF"
      },
      "source": [
        "## GPU backend"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_2Ib97_E13E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78357a14-48e2-46de-bbfc-8d6207e6f8c0"
      },
      "source": [
        "# %tensorflow_version 2.x\n",
        "# import tensorflow as tf\n",
        "# device_name = tf.test.gpu_device_name()\n",
        "# if device_name != '/device:GPU:0':\n",
        "#   raise SystemError('GPU device not found')\n",
        "# print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "# # install scikit-plot to Colab\n",
        "# !pip install scikit-plot"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-plot\n",
            "  Downloading scikit_plot-0.3.7-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: joblib>=0.10 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (1.0.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (0.22.2.post1)\n",
            "Requirement already satisfied: matplotlib>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (3.2.2)\n",
            "Requirement already satisfied: scipy>=0.9 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (1.4.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.19.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib>=1.4.0->scikit-plot) (1.15.0)\n",
            "Installing collected packages: scikit-plot\n",
            "Successfully installed scikit-plot-0.3.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VI8UC2kAvyAP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fba07be-4e91-40ce-c734-a41306ed8e80"
      },
      "source": [
        "# libraries import\n",
        "import os # miscellaneous operating system interfaces\n",
        "import numpy as np # Array library for linear algebra\n",
        "import matplotlib.pyplot as plt # visualization library\n",
        "from sklearn.metrics import classification_report # evaluators classification report\n",
        "import scikitplot as skplt # scikit-learn and matplotlib integrated visualization library\n",
        "from tensorflow import keras # back-end artificial neural network API\n",
        "from tensorflow.keras import layers # layer API for neural network architechture\n",
        "from tensorflow.keras.optimizers import Adam # optimizer in neural network implement\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau # Reduce learning rate when a metric has stopped improving\n",
        "from keras.models import model_from_json # loading model in JSON format\n",
        "\n",
        "# system configuration\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]=\"0\" # disable warning messages\n",
        "os.environ['AUTOGRAPH_VERBOSITY'] = '0' # set it to 0\n",
        "random_seed = 20 # random seed\n",
        "\n",
        "print('\\n')\n",
        "print('necessary libraries has successfully imported')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "necessary libraries has successfully imported\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_2nnZRlujmA",
        "outputId": "69ecbaa7-8d8a-4c92-db3e-e0e72f44a244"
      },
      "source": [
        "# connect to google drive to obtain dataset\n",
        "# !note: need to perform authorization by following below link\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mv0YbpOWzdOW",
        "outputId": "9956c811-4681-4c6f-9287-d38c7a7ec59e"
      },
      "source": [
        "# define model name\n",
        "modelName = 'MLP'\n",
        "\n",
        "print(modelName, 'modeling is initializing')\n",
        "print('\\n')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP modeling is initializing\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-Hrt5epgDiW"
      },
      "source": [
        "## Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dX9qHI4qO6kE"
      },
      "source": [
        "# define path to files\n",
        "defaultdir = '/content/drive/My Drive/all_about_phd_research/'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRW07lUT7yOO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa31f439-e249-42f6-8cad-953975c6041c"
      },
      "source": [
        "print('loading processed data')\n",
        "print('\\n')\n",
        "\n",
        "# load input data from drive\n",
        "X_train = np.load(defaultdir + '/processed_data/X_train.npy')\n",
        "X_test = np.load(defaultdir + '/processed_data/X_test.npy')\n",
        "y_train = np.load(defaultdir + '/processed_data/y_train.npy')\n",
        "y_test = np.load(defaultdir + '/processed_data/y_test.npy')\n",
        "\n",
        "print('loading done')\n",
        "print('\\n')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading processed data\n",
            "\n",
            "\n",
            "loading done\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncZK65QNPgom"
      },
      "source": [
        "## Data Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnL8kRTGPpET"
      },
      "source": [
        "# define function to create MLP model\n",
        "def mlpModel(data_train, num_classes, layer=1, units= 64, \n",
        "             activation= \"relu\", use_bias=True, drop_size=0.5, \n",
        "             ):\n",
        "    \n",
        "    # input layer convert arrays => tensors\n",
        "    inputs = keras.Input(data_train.shape[1:]) # dimension of the input\n",
        "\n",
        "    x1 = layers.Dense(units = units, \n",
        "                      activation= activation,\n",
        "                      use_bias= use_bias)(inputs)\n",
        "    x1 = layers.Dense(units = units, \n",
        "                      activation= activation,\n",
        "                      use_bias= use_bias)(x1)\n",
        "    x1 = layers.Dropout(drop_size)(x1)\n",
        "    \n",
        "    dense_size = units\n",
        "    \n",
        "    if layer > 1:\n",
        "        \n",
        "        x1 = layers.Dense(units = 2*units, \n",
        "                          activation= activation,\n",
        "                          use_bias= use_bias)(x1)\n",
        "        x1 = layers.Dense(units = 2*units, \n",
        "                          activation= activation,\n",
        "                          use_bias= use_bias)(x1)\n",
        "        x1 = layers.Dropout(drop_size)(x1)\n",
        "        \n",
        "        dense_size = 2*units\n",
        "        \n",
        "        if layer > 2:\n",
        "\n",
        "            x1 = layers.Dense(units = 4*units, \n",
        "                              activation= activation,\n",
        "                              use_bias= use_bias)(x1)\n",
        "            x1 = layers.Dense(filunitsters = 4*units, \n",
        "                              activation= activation,\n",
        "                              use_bias= use_bias)(x1)\n",
        "            x1 = layers.Dropout(drop_size)(x1)\n",
        "            \n",
        "            dense_size = 4*units\n",
        "            \n",
        "            if layer > 3:\n",
        "\n",
        "                x1 = layers.Dense(units = 8*units, \n",
        "                                  activation= activation,\n",
        "                                  use_bias= use_bias)(x1)\n",
        "                x1 = layers.Dense(units = 8*units, \n",
        "                                  activation= activation,\n",
        "                                  use_bias= use_bias)(x1)\n",
        "                x1 = layers.Dropout(drop_size)(x1)\n",
        "                \n",
        "                dense_size = 8*units\n",
        "                \n",
        "                if layer > 4:\n",
        "\n",
        "                    x1 = layers.Dense(units = 8*units, \n",
        "                                      activation= activation,\n",
        "                                      use_bias= use_bias)(x1)\n",
        "                    x1 = layers.Dense(units = 8*units, \n",
        "                                      activation= activation,\n",
        "                                      use_bias= use_bias)(x1)\n",
        "                    x1 = layers.Dropout(drop_size)(x1)\n",
        "                    \n",
        "                    dense_size = 8*units\n",
        "\n",
        "    x2 = layers.Flatten()(x1)\n",
        "    x2 = layers.Dense(dense_size, activation = activation)(x2)\n",
        "    # x2 = layers.Dense(dense_size, activation = activation)(x1)\n",
        "\n",
        "    outputs = layers.Dense(num_classes, activation= \"softmax\")(x2)\n",
        "\n",
        "    mlpModel = keras.Model(inputs=inputs, outputs=outputs, name=\"mlpModel\")\n",
        "\n",
        "    mlpModel.summary()\n",
        "    \n",
        "    return mlpModel"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYeQhYLvLV7X"
      },
      "source": [
        "# !!!warning: if assign layer > 1 will causing OOM (Out Of Memory) error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3tZx-_KTx_T"
      },
      "source": [
        "# perform MLP model define function\n",
        "mlpModel = mlpModel(X_train, layer=1, num_classes=3)\n",
        "\n",
        "print('\\n')\n",
        "print(modelName, 'model contruction is done')\n",
        "print('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tke4v3IJZxrV"
      },
      "source": [
        "# plot MLP model architecture\n",
        "# the figure will be saved in the same directory with input datas\n",
        "tf.keras.utils.plot_model(mlpModel, \n",
        "                          to_file=defaultdir + '/processed_data/{}_model_architecture.png'.format(modelName), \n",
        "                          show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZNcjoQDUAgk"
      },
      "source": [
        "# define ADAM opimizer & complie resulting in ready-to-train model function\n",
        "def modelComplie(mlpModel, X_train, y_train, \n",
        "             X_test, y_test,\n",
        "             epochs, batch_size, \n",
        "             learning_rate=0.001, beta_1=0.9, beta_2=0.999, \n",
        "             epsilon=1e-08, decay=0):\n",
        "\n",
        "  # Define the optimizer\n",
        "  optimizer = Adam(learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2, \n",
        "                   epsilon=epsilon, decay=decay)\n",
        "\n",
        "  # assign metrics for the model\n",
        "  mlpModel.compile(optimizer=optimizer , \n",
        "                   loss=\"categorical_crossentropy\", \n",
        "                   metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCobOoDxxGr9"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dj6YOvnn2aM0"
      },
      "source": [
        "# define training epoch number & batch size  parameters\n",
        "epochs_list = [5]\n",
        "batch_size_list = [5]\n",
        "\n",
        "# Set a learning rate annealer\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n",
        "                                          patience=3, \n",
        "                                          verbose=1, \n",
        "                                          factor=0.5, \n",
        "                                          min_lr=0.00001)\n",
        "  \n",
        "# loop for training model\n",
        "for epochs in epochs_list:\n",
        "  for batch_size in batch_size_list:\n",
        "\n",
        "    # perform compile model function\n",
        "    modelComplie(mlpModel=mlpModel, X_train=X_train, y_train=y_train, \n",
        "         X_test=X_test, y_test=y_test, \n",
        "         epochs=epochs, batch_size=batch_size)\n",
        "    \n",
        "    print('\\n')\n",
        "    print(modelName, 'model is training in total =', epochs, 'epochs')\n",
        "    print('In each epoch has batch size =', batch_size, 'samples')\n",
        "    print('\\n')\n",
        "    \n",
        "    \n",
        "    # training model with learning rate annealer\n",
        "    history = mlpModel.fit(X_train, y_train, \n",
        "                         batch_size=batch_size,\n",
        "                         epochs = epochs, \n",
        "                         validation_data = (X_test,y_test),\n",
        "                         verbose = 1, \n",
        "                         steps_per_epoch=X_train.shape[0]//batch_size,\n",
        "                         callbacks=[learning_rate_reduction])\n",
        "    \n",
        "    print('\\n')\n",
        "    print(modelName,'model training has done')\n",
        "    print('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bs5tGTk8xJ18"
      },
      "source": [
        "## Model Prediction\n",
        "### Note:\n",
        "At this stage, we have 5 datasets and 1 model which are\n",
        "1. X_train: training dataset\n",
        "2. X_test: testing dataset\n",
        "3. y_train: training label\n",
        "4. y_test: testing label\n",
        "5. y_pred: prediction label from model\n",
        "6. mlpModel: MLP model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qL_E_urkZgbM"
      },
      "source": [
        "print(modelName, 'model is making prediction')\n",
        "print('\\n')\n",
        "\n",
        "# making predictions\n",
        "y_pred = mlpModel.predict(X_test, verbose=1)\n",
        "\n",
        "print('\\n')\n",
        "print(modelName, 'model making prediction has done')\n",
        "print('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMjdzbfAzCwp"
      },
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rapgsts_2VG3"
      },
      "source": [
        "print(modelName, 'model evaluation is now processing')\n",
        "print('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVsuNNR4cLlf"
      },
      "source": [
        "print('loss & accuracy plot')\n",
        "print('\\n')\n",
        "\n",
        "# Plot the loss & accuracy curves for training and validation\n",
        "# Define number of plots -> 2\n",
        "figurename = 'Loss and Accuracy plot of {} model'.format(modelName)\n",
        "\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.tight_layout()\n",
        "\n",
        "# set font sizes\n",
        "SMALL_SIZE = 12\n",
        "MEDIUM_SIZE = 15\n",
        "BIGGER_SIZE = 20\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Loss plot\n",
        "plt.subplot(2,1,1)\n",
        "plt.plot(history.history['loss'], '--', color='b', label=\"training loss\")\n",
        "plt.plot(history.history['val_loss'], '-', color='b', label=\"validation loss\")\n",
        "\n",
        "# decorating\n",
        "plt.axis([0, 1, 0, 1]) # define lower & upper boundaries of axes ([x-axis, y-axis])\n",
        "legend = plt.legend(loc='center right', shadow=True, bbox_to_anchor=(1.375, 0.5), ncol=1,fontsize=MEDIUM_SIZE)\n",
        "plt.title('Loss plot', fontsize=BIGGER_SIZE)\n",
        "plt.tick_params(axis=\"x\", labelsize=15)\n",
        "plt.tick_params(axis=\"y\", labelsize=15)\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Accuracy plot\n",
        "plt.subplot(2,1,2)\n",
        "plt.plot(history.history['accuracy'], '--', color='r', label=\"training accuracy\")\n",
        "plt.plot(history.history['val_accuracy'], '-', color='r',label=\"validation accuracy\")\n",
        "\n",
        "# decorating\n",
        "plt.axis([0, 1, 0, 1]) # define lower & upper boundaries of axes ([x-axis, y-axis])\n",
        "legend = plt.legend(loc='lower right', shadow=True, bbox_to_anchor=(1.43, 0.5), ncol=1, fontsize=MEDIUM_SIZE)\n",
        "plt.title('Accuracy plot', fontsize=BIGGER_SIZE)\n",
        "plt.tick_params(axis=\"x\", labelsize=15)\n",
        "plt.tick_params(axis=\"y\", labelsize=15)\n",
        "\n",
        "# plt.subplots_adjust(hspace=0.5)\n",
        "\n",
        "# save figure\n",
        "plt.savefig(defaultdir + '/processed_data/' + figurename + '.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fPO8e-tWG4o"
      },
      "source": [
        "## Confusion matrix and AUC-ROC curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eyyo9x0tevO8"
      },
      "source": [
        "# def plot confusion matrix function\n",
        "def plotConfusionMatrix(y_test=y_test, y_pred=y_pred, cmap=plt.cm.Reds, title='Confusion Matrix of {} model'.format(modelName)):\n",
        "\n",
        "  # set font sizes\n",
        "  SMALL_SIZE = 12\n",
        "  MEDIUM_SIZE = 15\n",
        "  BIGGER_SIZE = 20\n",
        "\n",
        "  skplt.metrics.plot_confusion_matrix(y_true=np.argmax(y_test, axis=1), \n",
        "                                      y_pred=np.argmax(y_pred, axis=1), \n",
        "                                      title=title, \n",
        "                                      normalize='true', \n",
        "                                      figsize=(9,9), \n",
        "                                      cmap=cmap, \n",
        "                                      title_fontsize= BIGGER_SIZE, \n",
        "                                      text_fontsize= BIGGER_SIZE\n",
        "                                      )\n",
        "\n",
        "  # save figure\n",
        "  plt.savefig(defaultdir + '/processed_data/' + title + '.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5G7hiTmPfagm"
      },
      "source": [
        "print('confusion matrix plot')\n",
        "print('\\n')\n",
        "\n",
        "# perform plot confusion matrix function\n",
        "title = 'Confusion Matrix of {} model'.format(modelName)\n",
        "plot = plotConfusionMatrix(y_test=y_test, y_pred=y_pred, cmap=plt.cm.Reds, title=title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRnwzOPlipIj"
      },
      "source": [
        "# def plot ROC curve function\n",
        "def plotROC(y_test=y_test, y_pred=y_pred, cmap=plt.cm.Reds, title='ROC curve of {} model'.format(modelName)):\n",
        "\n",
        "  # set font sizes\n",
        "  SMALL_SIZE = 12\n",
        "  MEDIUM_SIZE = 15\n",
        "  BIGGER_SIZE = 20\n",
        "\n",
        "  skplt.metrics.plot_roc(y_true=np.argmax(y_test, axis=1), \n",
        "                                      y_probas=y_pred, \n",
        "                                      title=title, \n",
        "                                      plot_micro=True, \n",
        "                                      plot_macro=True, \n",
        "                                      figsize=(12,6), \n",
        "                                      cmap=cmap, \n",
        "                                      title_fontsize= BIGGER_SIZE, \n",
        "                                      text_fontsize= MEDIUM_SIZE\n",
        "                         )\n",
        "\n",
        "  # save figure\n",
        "  plt.savefig(defaultdir + '/processed_data/' + title + '.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ts3c6Z1LjhzN"
      },
      "source": [
        "print('AUC-ROC curve plot')\n",
        "print('\\n')\n",
        "\n",
        "# perform plot AUC-ROC curve function\n",
        "title = 'AUC-ROC curve of {} model'.format(modelName)\n",
        "plot = plotROC(y_test=y_test, y_pred=y_pred, cmap=plt.cm.Reds, title=title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lW2pUrx7nisj"
      },
      "source": [
        "## Print Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dceTyX8xmY2m"
      },
      "source": [
        "# classification report\n",
        "num_classes = 3\n",
        "class_list = ['class '+ str(y_class) for y_class in range(num_classes)]\n",
        "\n",
        "print('classification report of {} model'.format(modelName))\n",
        "print('\\n')\n",
        "print(classification_report(np.argmax(y_test,axis=1), \n",
        "                            np.argmax(y_pred,axis=1), \n",
        "                            target_names= class_list))\n",
        "\n",
        "print('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDVHXh5FqW_L"
      },
      "source": [
        "## Saving model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MfPsHTTpnSv"
      },
      "source": [
        "# define saving function\n",
        "def save_model (model, model_name):\n",
        "\n",
        "    # serialize model to JSON\n",
        "    model_json = model.to_json()\n",
        "    \n",
        "    with open(\"{}.json\".format(defaultdir + '/processed_data/' + model_name), \"w\") as json_file:\n",
        "        \n",
        "        json_file.write(model_json)\n",
        "\n",
        "        # serialize weights to HDF5\n",
        "        model.save_weights(\"{}_weight.h5\".format(defaultdir + '/processed_data/' + model_name))\n",
        "\n",
        "    print(\"Saved {} to drive\".format(model_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iOez2xFqt6x"
      },
      "source": [
        "# perform saving\n",
        "save_model(model=mlpModel, model_name= modelName.lower() + 'Model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsHpjeUzL93x"
      },
      "source": [
        "import sys\n",
        "sys.getsizeof(mlpModel.get_weights())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFka7ubuvimJ"
      },
      "source": [
        "## Load the saved model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gAU2tKPq7nY"
      },
      "source": [
        "# define loading function\n",
        "def load_model (model, model_name):\n",
        "\n",
        "    # load json and create model\n",
        "    json_file = open('{}.json'.format(defaultdir + '/processed_data/' + model_name), 'r')\n",
        "    loaded_model_json = json_file.read()\n",
        "    json_file.close()\n",
        "    loaded_model = model_from_json(loaded_model_json)\n",
        "\n",
        "    # load weights into new model\n",
        "    loaded_model.load_weights(\"{}_weight.h5\".format(defaultdir + '/processed_data/' + model_name))\n",
        "    print(\"Loaded {} from drive\".format(model_name))\n",
        "    \n",
        "    return loaded_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otktQAMVs2Ku"
      },
      "source": [
        "# perform loading\n",
        "mlpModel = load_model(model=mlpModel, model_name= modelName.lower() + 'Model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LlLXm-CvmUV"
      },
      "source": [
        "## Perform testing loaded model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EB4ch3AFtSnn"
      },
      "source": [
        "# perform compile model function\n",
        "modelComplie(mlpModel=mlpModel, X_train=X_train, y_train=y_train, \n",
        "         X_test=X_test, y_test=y_test, \n",
        "         epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "# evaluate the loaded model\n",
        "mlpModel.evaluate(X_train, y_train, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9xXcVgcvGV6"
      },
      "source": [
        "# making prediction of loaded model\n",
        "mlpModel.predict(X_test, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5zFBxl3wMnv"
      },
      "source": [
        "print('\\n')\n",
        "print(modelName, 'modeling process is completed')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXOAQkOywI1Z"
      },
      "source": [
        "# Finish Data life cycle"
      ]
    }
  ]
}